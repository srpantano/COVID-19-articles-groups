{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cord19.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOnOnKWQo+icAsCDNC9LaVE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srpantano/mestrado/blob/master/Cord19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgC52ltswfkv",
        "colab_type": "text"
      },
      "source": [
        "# BLIBLIOTECAS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvGqSyJwwUs-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import json\n",
        "import seaborn as sns\n",
        "import spacy\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6Mz2Ls0wm-G",
        "colab_type": "text"
      },
      "source": [
        "# FUNÇÕES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyyeHVJSwrkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_json_to_df(json_files, df):\n",
        "    \n",
        "    for file_names in json_files:\n",
        "        \n",
        "        #keywords ={'paper_id': None, 'title': None, 'abstract': None}\n",
        "        keywords ={'paper_id': None, 'abstract': None}\n",
        "        \n",
        "        with open(file_names) as json_data:            \n",
        "        \n",
        "            data = json.load(json_data)\n",
        "        \n",
        "            if 'paper_id' not in data: \n",
        "                keywords['paper_id'] = np.nan\n",
        "            else:\n",
        "                keywords['paper_id'] = data['paper_id'].strip() #Retira os espaço antes e depois \n",
        "        \n",
        "           #Armazena o Titulo\n",
        "            '''if 'metadata' not in data: \n",
        "                keywords['title'] = np.nan\n",
        "            else:\n",
        "                keywords['title'] = data['metadata']['title'].strip()'''\n",
        "        \n",
        "            #Armazena o Abstract\n",
        "            if 'abstract' not in data: \n",
        "                keywords['abstract'] = np.nan\n",
        "            else:\n",
        "                abstracts = [abstract['text'] for abstract in data['abstract']]\n",
        "                abstract = '\\n'.join(abstracts)\n",
        "                keywords['abstract'] = abstract.strip()\n",
        "            \n",
        "            df = df.append(keywords, ignore_index = True)\n",
        "        \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c7d6A0Jwvk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(sentence): \n",
        "    #Todo texto em minúsculo\n",
        "    sentence = sentence.lower()\n",
        "    list_ = []\n",
        "    for word in nlp(sentence):\n",
        "        \n",
        "        #Retirar stopword, numeral, pontuação, espaço duplo \n",
        "        if not (word.is_stop or word.like_num or word.is_punct or word.is_space or \n",
        "                len(word) == 1):\n",
        "            \n",
        "            #Realiza a lematização \n",
        "            list_.append(word.lemma_)\n",
        "            \n",
        "    list_ = list( dict.fromkeys(list_) )\n",
        "            \n",
        "    return ' '.join([str(element) for element in list_])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53iA-6CRw4eF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Atrinbutos a serem coletados\n",
        "papers_features = {'paper_id': [], 'abstract': []}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iob99KV7w7z8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Trasforma o dicionário em Dataframe\n",
        "df = pd.DataFrame.from_dict(papers_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWt0d_etw84a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lê todos os arquivos .json do diretorio\n",
        "json_filenames = glob.glob(f'{\"C:/Users/sergi/Documents/Cord-19/document_parses\"}//**/*.json', recursive = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijKA5jBZw_q6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Salva os artigos em Dataframe\n",
        "df = load_json_to_df(json_filenames, df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmirvH1PxJJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df = pd.read_csv('C:/Users/sergi/Documents/Cord-19/cord_df_reduzido.csv')\n",
        "\n",
        "#df.to_csv('C:/Users/sergi/Documents/Cord-19/cord_df_reduzido.csv', index=False, header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns5MyUDHxM2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plota heatmap de nulos\n",
        "sns.heatmap(df.notnull(), cmap=\"Blues\");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IiGlDacxWhG",
        "colab_type": "text"
      },
      "source": [
        "# Pré-processamento da base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WGfgPJxxRie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cleared = df.dropna(subset=['abstract'])\n",
        "df_cleared = df_cleared[df_cleared['abstract'] != '']\n",
        "df_cleared = df_cleared.drop_duplicates(['abstract'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr21mlSYxgMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'ner'])\n",
        "df_cleared = df_cleared.assign(abstract=df_cleared['abstract'].apply(normalize))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr2OlynLxuFf",
        "colab_type": "text"
      },
      "source": [
        "# Análise da base de Abstracts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydYb6qunx1-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cleared['word_count'] = df_cleared['abstract'].apply(lambda x: len(str(x).split(\" \")))\n",
        "print(\"Mediana de quantidade de palavras no Abstract por artigo: \" + str(df_cleared['word_count'].median()))\n",
        "print(df_cleared['word_count'].describe())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UofORJmox4g5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = df_cleared['word_count']\n",
        "n_bins = 20\n",
        "plt.hist(x, bins=n_bins)\n",
        "plt.xlabel('Número de palavras no Abstract')\n",
        "plt.ylabel('Frequência')\n",
        "plt.show()\n",
        "\n",
        "boxplot = df_cleared.boxplot(column=['word_count'])\n",
        "\n",
        "df_wc700 = df_cleared.loc[(df_cleared['word_count'] > 10) & (df_cleared['word_count'] < 400)]\n",
        "print(\"Mediana de quantidade de palavras no Abstract por artigo: \" + str(df_wc700['word_count'].median()))\n",
        "print(df_wc700['word_count'].describe())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oRMAj8YyDnw",
        "colab_type": "text"
      },
      "source": [
        "# TOKENIZAÇÃO E PALAVRAS MAIS FREQUENTES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_d00NIbx7a-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Obtem a lista de textos\n",
        "abstracts = df_wc700['abstract'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMeACXF3yR3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cria um vocabulário de palavras, eleminado as palavras que aparecem em\n",
        "#mais de 85% dos documentos e os stop word em inglês\n",
        "#a principio retornou um vocabuário de 164.179 palavras, dificultando a clusterização.\n",
        "#Para reduzir foi utilizado o parametro max_features, reduzindo para 4096 palavras\n",
        "#cv=CountVectorizer(max_df=0.50, max_features=2**20, stop_words = 'english')\n",
        "cv=CountVectorizer()\n",
        "word_count_vector=cv.fit_transform(abstracts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khi0bmeTyUA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sum_words = word_count_vector.sum(axis=0)\n",
        "\n",
        "words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n",
        "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oNClZJryXhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lista das palavras mais frequentes\n",
        "most_freqs = words_freq[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwn2MkZMyaeH",
        "colab_type": "text"
      },
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8LJzay4yZzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Ordena a matrix\n",
        "def sort_coo(coo_matrix):\n",
        "    #Array de tuplas (coluna, valor)\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data) \n",
        "    #Ordena em ordem decrescente \n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "\n",
        "#Extrai os mais relevantes\n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "    \n",
        "    sorted_items = sorted_items[:topn]\n",
        " \n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "    \n",
        "    for idx, score in sorted_items:\n",
        "        \n",
        "        score_vals.append(round(score, 3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        " \n",
        "    results= {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "    \n",
        "    return results\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "vectorized = tfidf.fit_transform(abstracts)\n",
        "\n",
        "sorted_items=sort_coo(vectorized.tocoo())\n",
        "\n",
        "feature_names= tfidf.get_feature_names()\n",
        "keywords=extract_topn_from_vector(feature_names, sorted_items, 42215)\n",
        "\n",
        "more_import_words = tfidf.inverse_transform(vectorized)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk6HyloWys6T",
        "colab_type": "text"
      },
      "source": [
        "# K-Means"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrZbegdBysXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calcula o número ideal de clusters, baseado no método Elbow\n",
        "wcss = []\n",
        "for i in range(1, 20):\n",
        "    print(i)\n",
        "    kmeans = MiniBatchKMeans(init='k-means++', n_clusters=i, max_iter=300, n_init=10, \n",
        "                         random_state=6)\n",
        "    kmeans.fit(vectorized)\n",
        "    wcss.append(kmeans.inertia_) #inertia = soma dos quadrados das distâncias dos pontos para o centróide\n",
        "\n",
        "plt.plot(range(1, 20), wcss)\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('WCSS')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9H0KQNq5skc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calcula o número ideal de clusters\n",
        "def optimal_number_of_clusters(wcss):\n",
        "    x1, y1 = 1, wcss[0]\n",
        "    x2, y2 = 20, wcss[len(wcss)-1]\n",
        "\n",
        "    distances = []\n",
        "    for i in range(len(wcss)):\n",
        "        x0 = i+2\n",
        "        y0 = wcss[i]\n",
        "        numerator = abs((y2-y1)*x0 - (x2-x1)*y0 + x2*y1 - y2*x1)\n",
        "        denominator = np.sqrt((y2 - y1)**2 + (x2 - x1)**2)\n",
        "        distances.append(numerator/denominator)\n",
        "    \n",
        "    return distances.index(max(distances)) + 2\n",
        "\n",
        "print(optimal_number_of_clusters(wcss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnGIa3rc6Qbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reduz para 2 dimensões para gerar o gráfico\n",
        "pca = PCA(n_components = 9)\n",
        "X_pca = pca.fit_transform(vectorized.toarray())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9tfMAEI6MsZ",
        "colab_type": "text"
      },
      "source": [
        "MiniBatchKMeans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoYNTTJu5uk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kmeans = MiniBatchKMeans(init='k-means++', n_clusters=9, max_iter=300, n_init=10, \n",
        "                         random_state=6)\n",
        "\n",
        "y_pred = kmeans.fit_predict(vectorized)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAIP1qQ96Ybk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=plt.figaspect(0.5))\n",
        "\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "ax.scatter(X_pca[:,0], X_pca[:,1], c=kmeans.predict(vectorized))\n",
        "\n",
        "centers_on_pcs = pca.transform(kmeans.cluster_centers_)\n",
        "ax.scatter(centers_on_pcs[:, 0], centers_on_pcs[:,1], marker='o', s=150, edgecolor='k')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxFZazPj6c6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Demonstra as palavras mais importantes do cluster\n",
        "#https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html\n",
        "#Retorna os centroides em um array ordenado por valor  \n",
        "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1] #[:, ::-1] inverte o valor\n",
        "\n",
        "terms = tfidf.get_feature_names()\n",
        "for i in range(9):\n",
        "    print(\"Cluster %d:\" % i, end='')\n",
        "    for ind in order_centroids[i, :5]:\n",
        "        print(' %s' % terms[ind], end='')\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T96RcXoB6e8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Gráfico dos clusters\n",
        "palette = sns.color_palette('bright', len(set(y_pred)))\n",
        "sns.scatterplot(X_pca[:,0], X_pca[:, 1], hue=y_pred, legend='full', palette=palette)\n",
        "plt.title('Clustered Covid-19 Papers');\n",
        "centers_on_pcs = pca.transform(kmeans.cluster_centers_)\n",
        "plt.scatter(x=centers_on_pcs[:,0], y=centers_on_pcs[:,1], s=100, c=\"k\", marker=\"X\")\n",
        "print(kmeans.labels_)\n",
        "df_wc700['y'] = y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5blaoosT6gzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Distância euclidiana entre os clusters\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "dists = euclidean_distances(kmeans.cluster_centers_)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}